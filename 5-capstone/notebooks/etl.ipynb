{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    FloatType,\n",
    "    StructType,\n",
    "    StructField\n",
    ")\n",
    "\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('spark.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "TESTING = bool(int(config['TEST']['TEST']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Initialize a Spark session.\n",
    "\n",
    "    Returns:\n",
    "        A Spark session.\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.hadoop:hadoop-aws:2.7.0\")\n",
    "    .getOrCreate()\n",
    "             )\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def replace_values(df: DataFrame,\n",
    "                   replace_dict: dict,\n",
    "                   column_name: str\n",
    "                   ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Replaces values on a spark dataframe's column with a dictionary.\n",
    "\n",
    "    Args:\n",
    "        df: A Spark DataFrame.\n",
    "        replace_dict: The replacement dictionary.\n",
    "        column_name: The column name to replace the values in.\n",
    "\n",
    "    Returns:\n",
    "        The Dataframe with tehe values replaced.\n",
    "\n",
    "    \"\"\"\n",
    "    mapping = F.create_map([F.lit(x) for x in chain(*replace_dict.items())])\n",
    "    df = df.withColumn(column_name, mapping[df[column_name]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def format_dates(df: DataFrame,\n",
    "                 column_name: str\n",
    "                 ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Formats a SAS date. SAS dates are an integer representing the number of days since 1960-01-01.\n",
    "    Args:\n",
    "        df: A Spark DataFrame.\n",
    "        column_name: The column name to replace the values in.\n",
    "\n",
    "    Returns:\n",
    "        The DataFrame with the dates parsed as date type.\n",
    "\n",
    "    \"\"\"\n",
    "    df = df.withColumn(column_name, F.expr(f\"date_add('1960-01-01', {column_name})\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immigration_data(spark: SparkSession,\n",
    "                             raw_data_path: str = None,\n",
    "                             destination_path: str = None\n",
    "                             ):\n",
    "    \"\"\"\n",
    "    Processes immigration data to create the fact and dimension tables.\n",
    "\n",
    "    Args:\n",
    "        spark: Spark session.\n",
    "        raw_data_path: Route to read the file from.\n",
    "        destination_path: Route to write the data to.\n",
    "    \"\"\"\n",
    "\n",
    "    data_path = \"immigration_data/\" if TESTING else f\"{raw_data_path}/immigration_data/\"\n",
    "    destination_path = \"data_test\" if TESTING else destination_path\n",
    "\n",
    "    immigration_cols_to_keep = {\n",
    "        \"cicid\": \"immigration_id\",\n",
    "        \"admnum\": \"admission_number\",\n",
    "        \"i94port\": \"port_code\",\n",
    "        \"fltno\": \"flight_number\",\n",
    "        \"visatype\": \"visa_type\",\n",
    "        \"airline\": \"airline\",\n",
    "        \"i94bir\": \"age\",\n",
    "        \"i94cit\": \"citizenship_country\",\n",
    "        \"i94res\": \"residency_country\",\n",
    "        \"arrdate\": \"arrival_date\",\n",
    "        \"depdate\": \"departure_date\",\n",
    "        \"visapost\": \"department_state_visa_issued\",\n",
    "        \"i94visa\": \"visa_code\",\n",
    "        \"visatype\": \"visa_type\",\n",
    "        \"i94mode\": \"transportation_mode\",\n",
    "        \"occup\": \"occupation\",\n",
    "        \"gender\": \"gender\"\n",
    "    }\n",
    "\n",
    "    print(\"Reading data\")\n",
    "    immigration_df = spark.read.parquet(data_path)\n",
    "    immigration_df = (immigration_df\n",
    "                      .select(*[F.col(col_name).alias(immigration_cols_to_keep.get(col_name, col_name))\n",
    "                                for col_name in immigration_cols_to_keep.keys()])\n",
    "                      )\n",
    "    # Fix coded columns\n",
    "    print(\"Replacing coded values in columns\")\n",
    "    immigration_df = replace_values(immigration_df, COUNTRY_CODES, \"citizenship_country\")\n",
    "    immigration_df = replace_values(immigration_df, COUNTRY_CODES, \"residency_country\")\n",
    "    immigration_df = replace_values(immigration_df, VISA_CODES, \"visa_code\")\n",
    "    immigration_df = replace_values(immigration_df, TRANSPORTATION_MODES, \"transportation_mode\")\n",
    "\n",
    "    # Format date columns\n",
    "    print(\"Formating date columns\")\n",
    "    immigration_df = format_dates(immigration_df, \"arrival_date\")\n",
    "    immigration_df = format_dates(immigration_df, \"departure_date\")\n",
    "\n",
    "    # Create immigration fact table\n",
    "    print(\"Creating immigration fact table\")\n",
    "    immigration_fact_cols = ['immigration_id', 'flight_number', 'port_code']\n",
    "    immigration_fact_df = immigration_df.select(immigration_fact_cols).dropDuplicates()\n",
    "    immigration_fact_df = (immigration_df\n",
    "                           .withColumn(\"immigration_id\", immigration_fact_df.immigration_id.cast('int'))\n",
    "                           )\n",
    "    (immigration_df\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('port_code')\n",
    "     .parquet(os.path.join(destination_path, \"immigration_fact.parquet\"))\n",
    "     )\n",
    "\n",
    "    # Create fligths dimension table\n",
    "    print(\"Creating flights dimension table\")\n",
    "    flights_dim_cols = ['flight_number', 'airline']\n",
    "    flights_dim_df = (immigration_df\n",
    "                      .select(flights_dim_cols)\n",
    "                      .dropDuplicates()\n",
    "                      .where(F.col(\"flight_number\").isNotNull())\n",
    "                      )\n",
    "    (flights_dim_df\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('airline')\n",
    "     .parquet(os.path.join(destination_path, \"flights_dim.parquet\"))\n",
    "     )\n",
    "\n",
    "    # Create visitors dimension table\n",
    "    print(\"Creating visitors dimension table\")\n",
    "    visitors_dim_cols = ['immigration_id', 'residency_country', 'citizenship_country', 'age', 'gender', 'occupation']\n",
    "    visitors_dim_df = immigration_df.select(visitors_dim_cols).dropDuplicates()\n",
    "    visitors_dim_df = (visitors_dim_df.withColumn('age', visitors_dim_df.age.cast('int'))\n",
    "                       .withColumn(\"immigration_id\", visitors_dim_df.immigration_id.cast('int'))\n",
    "                       )\n",
    "    (visitors_dim_df\n",
    "     .write.mode('overwrite')\n",
    "     .parquet(os.path.join(destination_path, \"visitors_dim.parquet\"))\n",
    "     )\n",
    "\n",
    "    # Create dates dimension table\n",
    "    print(\"Creating dates dimension table\")\n",
    "    dates_dim_cols = ['immigration_id', 'arrival_date', 'departure_date']\n",
    "    dates_dim_df = immigration_df.select(dates_dim_cols).dropDuplicates()\n",
    "    dates_dim_df = (dates_dim_df.withColumn(\"immigration_id\", dates_dim_df.immigration_id.cast('int'))\n",
    "                    .withColumn('arrival_year', F.year(dates_dim_df.arrival_date))\n",
    "                    .withColumn('arrival_month', F.month(dates_dim_df.arrival_date))\n",
    "                    .withColumn('arrival_day', F.dayofmonth(dates_dim_df.arrival_date))\n",
    "                    .withColumn('departure_year', F.year(dates_dim_df.departure_date))\n",
    "                    .withColumn('departure_month', F.month(dates_dim_df.departure_date))\n",
    "                    .withColumn('departure_day', F.dayofmonth(dates_dim_df.departure_date))\n",
    "                    )\n",
    "\n",
    "    (dates_dim_df\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('arrival_year', 'arrival_month')\n",
    "     .parquet(os.path.join(destination_path, \"dates_dim.parquet\"))\n",
    "     )\n",
    "\n",
    "    # Create visas dimension table\n",
    "    print(\"Creating visas dimension table\")\n",
    "    visas_dim_cols = ['immigration_id', 'visa_type', 'visa_code', 'department_state_visa_issued']\n",
    "    visas_dim_df = immigration_df.select(visas_dim_cols).dropDuplicates()\n",
    "    visas_dim_df = (visas_dim_df\n",
    "                    .withColumn(\"immigration_id\", visas_dim_df.immigration_id.cast('int'))\n",
    "                    )\n",
    "    (visas_dim_df\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('visa_type')\n",
    "     .parquet(os.path.join(destination_path, \"visas_dim.parquet\"))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_port_codes_data(spark: SparkSession,\n",
    "                            raw_data_path: str = None,\n",
    "                            destination_path: str = None\n",
    "                            ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes the port codes data to generate the port_codes dimension table.\n",
    "\n",
    "    Args:\n",
    "        spark: Spark session.\n",
    "        raw_data_path: Route to read the file from.\n",
    "        destination_path: Route to write the data to.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the port_codes data.\n",
    "    \"\"\"\n",
    "    data_path = \"i94_port_codes.csv\" if TESTING else f\"{raw_data_path}/i94_port_codes.csv\"\n",
    "    destination_path = \"data_test\" if TESTING else destination_path\n",
    "\n",
    "    port_codes_df = pd.read_csv(data_path)\n",
    "    print(\"Creating port codes dimension\")\n",
    "    port_codes_dim = spark.createDataFrame(port_codes_df)\n",
    "    (port_codes_dim\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('port_code')\n",
    "     .parquet(os.path.join(destination_path, \"port_codes_dim.parquet\"))\n",
    "     )\n",
    "\n",
    "    return port_codes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_demographics_data(spark: SparkSession,\n",
    "                              port_codes_df: pd.DataFrame,\n",
    "                              raw_data_path: str = None,\n",
    "                              destination_path: str = None\n",
    "                              ) -> None:\n",
    "    \"\"\"\n",
    "    Processes the demographics data to generate the general demographics and the race demographics dimensions.\n",
    "\n",
    "    Args:\n",
    "        spark: Spark session.\n",
    "        port_codes_df: A pandas DataFrame with the port_codes to generate the primary keys of the demographics tables.\n",
    "        raw_data_path: Route to read the file from.\n",
    "        destination_path: Route to write the data to\n",
    "    \"\"\"\n",
    "    data_path = \"us-cities-demographics.csv\" if TESTING else f\"{raw_data_path}/us-cities-demographics.csv\"\n",
    "    destination_path = \"data_test\" if TESTING else destination_path\n",
    "\n",
    "    race_dict = {'American Indian and Alaska Native': 'native_count',\n",
    "                 'Asian': 'asian_count',\n",
    "                 'Black or African-American': 'black_count',\n",
    "                 'Hispanic or Latino': 'hispanic_count',\n",
    "                 'White': 'white_count'\n",
    "                 }\n",
    "    # Process general demographics data\n",
    "    us_cities_df = pd.read_csv(data_path, sep=\";\")\n",
    "    us_cities_df.columns = [col.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in us_cities_df.columns]\n",
    "    demographics_df = (us_cities_df[['state_code', 'state',\n",
    "                                     'city', 'median_age',\n",
    "                                     'male_population', 'female_population',\n",
    "                                     'total_population', 'number_of_veterans',\n",
    "                                     'foreign_born', 'average_household_size',\n",
    "                                     ]]\n",
    "                       .drop_duplicates(subset=\"city\")\n",
    "                       )\n",
    "    port_codes_df['city'] = demographics_df['city'].str.lower()\n",
    "    demographics_df['city'] = demographics_df['city'].str.lower()\n",
    "    demographics_df = demographics_df.merge(port_codes_df[['city', 'port_code']], left_on='city', right_on='city',\n",
    "                                            how='inner')\n",
    "    demographics_df['city'] = demographics_df['city'].str.title()\n",
    "    demographics_df = demographics_df[['port_code', 'state_code', 'state',\n",
    "                                       'city', 'median_age',\n",
    "                                       'male_population', 'female_population',\n",
    "                                       'total_population', 'number_of_veterans',\n",
    "                                       'foreign_born', 'average_household_size',\n",
    "                                       ]]\n",
    "\n",
    "    # Create general demographics dimension table\n",
    "    print(\"Creating general demographics dimension\")\n",
    "    general_demographics_dim = spark.createDataFrame(demographics_df)\n",
    "    general_demographics_dim = (\n",
    "        general_demographics_dim.withColumn(\"male_population\", general_demographics_dim.male_population.cast('int'))\n",
    "            .withColumn(\"female_population\", general_demographics_dim.female_population.cast('int'))\n",
    "            .withColumn(\"total_population\", general_demographics_dim.total_population.cast('int'))\n",
    "            .withColumn(\"number_of_veterans\", general_demographics_dim.number_of_veterans.cast('int'))\n",
    "            .withColumn(\"foreign_born\", general_demographics_dim.foreign_born.cast('int'))\n",
    "    )\n",
    "    (general_demographics_dim\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('port_code')\n",
    "     .parquet(os.path.join(destination_path, \"general_demog_dim.parquet\"))\n",
    "     )\n",
    "\n",
    "    # Process race demographics data\n",
    "    race_demographics_df = (us_cities_df[['city', 'state', 'state_code', 'race', 'count']]\n",
    "                            .pivot_table('count', ['city', 'state', 'state_code'], 'race')\n",
    "                            .reset_index()\n",
    "                            )\n",
    "\n",
    "    race_demographics_df = race_demographics_df.drop_duplicates(subset=\"city\")\n",
    "    race_demographics_df.rename(columns=race_dict, inplace=True)\n",
    "    race_demographics_df['city'] = race_demographics_df['city'].str.lower()\n",
    "    race_demographics_df = race_demographics_df.merge(port_codes_df[['city', 'port_code']], left_on='city',\n",
    "                                                      right_on='city', how='inner')\n",
    "    race_demographics_df['city'] = race_demographics_df['city'].str.title()\n",
    "    race_demographics_df = race_demographics_df[['port_code', 'state_code', 'state', 'city',\n",
    "                                                 'native_count', 'asian_count',\n",
    "                                                 'black_count', 'hispanic_count', 'white_count'\n",
    "                                                 ]]\n",
    "\n",
    "    # Create race demographics dimension table\n",
    "    print(\"Creating race demographics dimension\")\n",
    "    race_demographics_dim = spark.createDataFrame(race_demographics_df)\n",
    "    race_demographics_dim = (\n",
    "        race_demographics_dim.withColumn(\"native_count\", race_demographics_dim.native_count.cast('int'))\n",
    "            .withColumn(\"asian_count\", race_demographics_dim.asian_count.cast('int'))\n",
    "            .withColumn(\"black_count\", race_demographics_dim.black_count.cast('int'))\n",
    "            .withColumn(\"hispanic_count\", race_demographics_dim.hispanic_count.cast('int'))\n",
    "            .withColumn(\"white_count\", race_demographics_dim.white_count.cast('int'))\n",
    "    )\n",
    "    (race_demographics_dim\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('port_code')\n",
    "     .parquet(os.path.join(destination_path, \"race_demog_dim.parquet\"))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_airports_data(spark: SparkSession,\n",
    "                          port_codes_df: pd.DataFrame,\n",
    "                          raw_data_path: str = None,\n",
    "                          destination_path: str = None\n",
    "                          ) -> None:\n",
    "    \"\"\"\n",
    "    Processes the airports data to generate the airports dimension.\n",
    "\n",
    "    Args:\n",
    "        spark: Spark session.\n",
    "        port_codes_df: A pandas DataFrame with the port_codes to generate the primary keys of the demographics tables.\n",
    "        raw_data_path: Route to read the file from.\n",
    "        destination_path: Route to write the data to\n",
    "    \"\"\"\n",
    "    data_path = \"airport-codes_csv.csv\" if TESTING else f\"{raw_data_path}/airport-codes_csv.csv\"\n",
    "    destination_path = \"data_test\" if TESTING else destination_path\n",
    "\n",
    "    airport_cast_types = {\n",
    "        \"elevation_ft\": \"float64\",\n",
    "        \"x_coordinate\": \"float64\",\n",
    "        \"y_coordinate\": \"float64\"\n",
    "    }\n",
    "    # Process airpots data\n",
    "    airport_df = pd.read_csv(data_path)\n",
    "    airport_df = airport_df[(airport_df[\"iso_country\"] == \"US\")]\n",
    "    airport_df = airport_df[(airport_df[\"type\"] != \"closed\")]\n",
    "    airport_df = airport_df[airport_df['municipality'].notnull()]\n",
    "    airport_df[\"state\"] = airport_df[\"iso_region\"].str.split(\"-\").str[1]\n",
    "    airport_df[['x_coordinate', 'y_coordinate']] = airport_df.coordinates.str.split(',', expand=True)\n",
    "    airport_df = airport_df[['local_code', 'iata_code',\n",
    "                             'type', 'name', 'state',\n",
    "                             'municipality', 'elevation_ft',\n",
    "                             'x_coordinate', 'y_coordinate'\n",
    "                             ]]\n",
    "    port_codes_df['city'] = port_codes_df['city'].str.lower()\n",
    "    airport_df['municipality'] = airport_df['municipality'].str.lower()\n",
    "    airport_df = airport_df.merge(port_codes_df[['city', 'port_code']], left_on='municipality', right_on='city',\n",
    "                                  how='inner')\n",
    "    airport_df['city'] = airport_df['city'].str.title()\n",
    "\n",
    "    airport_df = airport_df.astype(airport_cast_types)\n",
    "    airport_df = airport_df[['port_code', 'local_code',\n",
    "                             'iata_code', 'type', 'name',\n",
    "                             'state', 'city', 'elevation_ft',\n",
    "                             'x_coordinate', 'y_coordinate'\n",
    "                             ]]\n",
    "\n",
    "    # Create airports dimension table\n",
    "    print(\"Creating airports dimension table\")\n",
    "    schema = StructType([\n",
    "        StructField(\"port_code\", StringType(), True),\n",
    "        StructField(\"local_code\", StringType(), True),\n",
    "        StructField(\"iata_code\", StringType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"elevation_ft\", FloatType(), True),\n",
    "        StructField(\"x_coordinate\", FloatType(), True),\n",
    "        StructField(\"y_coordinte\", FloatType(), True),\n",
    "    ])\n",
    "    airports_dim = spark.createDataFrame(airport_df, schema=schema)\n",
    "    airports_dim = airports_dim.withColumn(\"elevation_ft\", airports_dim.elevation_ft.cast('int'))\n",
    "\n",
    "    (airports_dim\n",
    "     .write.mode('overwrite')\n",
    "     .partitionBy('port_code')\n",
    "     .parquet(os.path.join(destination_path, \"airports_dim.parquet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating port codes dimension\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "port_codes_df = process_port_codes_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating general demographics dimension\n",
      "Creating race demographics dimension\n"
     ]
    }
   ],
   "source": [
    "process_demographics_data(spark, port_codes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating airports dimension table\n"
     ]
    }
   ],
   "source": [
    "process_airports_data(spark, port_codes_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}